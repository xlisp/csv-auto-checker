#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Spark SQLéªŒè¯å™¨
æ¥å—Spark SQLè¯­å¥å’Œä¸¤ä¸ªæ–‡ä»¶ä½œä¸ºå‚æ•°ï¼Œæ¯”å¯¹è½¬æ¢ç»“æœå¹¶ç”ŸæˆHTMLæŠ¥è¡¨
"""

import argparse
import os
import sys
import json
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hash, md5, concat_ws
import pandas as pd
from typing import Dict, List, Tuple, Any

class SparkSQLValidator:
    def __init__(self, app_name="SparkSQL_Validator"):
        try:
            # åˆå§‹åŒ–Sparkä¼šè¯
            self.spark = SparkSession.builder \
                .appName(app_name) \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.sql.shuffle.partitions", "200") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("WARN")
            self.comparison_results = {}
            self.execution_log = []
            self.log_message("Sparkä¼šè¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"Sparkä¼šè¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            sys.exit(1)
    
    def log_message(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        print(log_entry)
        self.execution_log.append(log_entry)
    
    def read_sql_from_file(self, sql_path: str) -> str:
        """ä»æ–‡ä»¶è¯»å–SQLè¯­å¥"""
        try:
            if not os.path.exists(sql_path):
                raise FileNotFoundError(f"SQLæ–‡ä»¶ä¸å­˜åœ¨: {sql_path}")
            
            with open(sql_path, 'r', encoding='utf-8') as f:
                sql_content = f.read().strip()
            
            self.log_message(f"æˆåŠŸè¯»å–SQLæ–‡ä»¶: {sql_path}")
            return sql_content
            
        except Exception as e:
            self.log_message(f"è¯»å–SQLæ–‡ä»¶å¤±è´¥ {sql_path}: {str(e)}", "ERROR")
            raise
    
    def read_csv_file(self, file_path: str, table_name: str) -> bool:
        """è¯»å–CSVæ–‡ä»¶å¹¶æ³¨å†Œä¸ºä¸´æ—¶è¡¨"""
        try:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            self.log_message(f"å¼€å§‹è¯»å–CSVæ–‡ä»¶: {file_path}")
            
            df = self.spark.read \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .option("encoding", "UTF-8") \
                .option("multiline", "true") \
                .option("escape", '"') \
                .csv(file_path)
            
            # æ³¨å†Œä¸´æ—¶è¡¨
            df.createOrReplaceTempView(table_name)
            
            row_count = df.count()
            col_count = len(df.columns)
            
            self.log_message(f"æˆåŠŸè¯»å–æ–‡ä»¶ {file_path} -> è¡¨ {table_name}")
            self.log_message(f"æ•°æ®ç»´åº¦: {row_count} è¡Œ x {col_count} åˆ—")
            self.log_message(f"å­—æ®µåˆ—è¡¨: {', '.join(df.columns)}")
            
            return True
            
        except Exception as e:
            self.log_message(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}", "ERROR")
            return False
    
    def execute_spark_sql(self, sql_query: str, result_table: str = "transform_result"):
        """æ‰§è¡ŒSpark SQLå¹¶è¿”å›ç»“æœDataFrame"""
        try:
            self.log_message("å¼€å§‹æ‰§è¡ŒSpark SQLè½¬æ¢...")
            self.log_message(f"SQLè¯­å¥:\n{sql_query}")
            
            # æ‰§è¡ŒSQL
            result_df = self.spark.sql(sql_query)
            
            # æ³¨å†Œç»“æœè¡¨
            result_df.createOrReplaceTempView(result_table)
            
            row_count = result_df.count()
            col_count = len(result_df.columns)
            
            self.log_message(f"SQLæ‰§è¡ŒæˆåŠŸï¼Œç»“æœç»´åº¦: {row_count} è¡Œ x {col_count} åˆ—")
            self.log_message(f"ç»“æœå­—æ®µ: {', '.join(result_df.columns)}")
            
            return result_df
            
        except Exception as e:
            self.log_message(f"SQLæ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def load_expected_results(self, expected_file: str, expected_table: str = "expected_result"):
        """åŠ è½½æœŸæœ›ç»“æœæ–‡ä»¶"""
        try:
            if not self.read_csv_file(expected_file, expected_table):
                raise Exception("æ— æ³•è¯»å–æœŸæœ›ç»“æœæ–‡ä»¶")
            
            expected_df = self.spark.table(expected_table)
            self.log_message("æœŸæœ›ç»“æœæ–‡ä»¶åŠ è½½æˆåŠŸ")
            
            return expected_df
            
        except Exception as e:
            self.log_message(f"åŠ è½½æœŸæœ›ç»“æœå¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def compare_schemas(self, actual_df, expected_df) -> Dict[str, Any]:
        """æ¯”è¾ƒæ•°æ®ç»“æ„"""
        actual_schema = {field.name: str(field.dataType) for field in actual_df.schema.fields}
        expected_schema = {field.name: str(field.dataType) for field in expected_df.schema.fields}
        
        actual_columns = set(actual_schema.keys())
        expected_columns = set(expected_schema.keys())
        
        schema_comparison = {
            "actual_columns": sorted(actual_columns),
            "expected_columns": sorted(expected_columns),
            "missing_columns": sorted(expected_columns - actual_columns),
            "extra_columns": sorted(actual_columns - expected_columns),
            "common_columns": sorted(actual_columns & expected_columns),
            "data_type_matches": {},
            "data_type_mismatches": {}
        }
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        for col_name in schema_comparison["common_columns"]:
            actual_type = actual_schema[col_name]
            expected_type = expected_schema[col_name]
            
            if actual_type == expected_type:
                schema_comparison["data_type_matches"][col_name] = actual_type
            else:
                schema_comparison["data_type_mismatches"][col_name] = {
                    "actual": actual_type,
                    "expected": expected_type
                }
        
        return schema_comparison
    
    def compare_data_content(self, actual_df, expected_df, common_columns: List[str]) -> Dict[str, Any]:
        """æ¯”è¾ƒæ•°æ®å†…å®¹"""
        try:
            # åªæ¯”è¾ƒå…±åŒåˆ—
            actual_subset = actual_df.select(*common_columns)
            expected_subset = expected_df.select(*common_columns)
            
            actual_count = actual_subset.count()
            expected_count = expected_subset.count()
            
            # è®¡ç®—è¡Œçº§åˆ«çš„å“ˆå¸Œå€¼è¿›è¡Œæ¯”è¾ƒ
            actual_with_hash = actual_subset.withColumn(
                "row_hash", 
                md5(concat_ws("|", *[col(c).cast("string") for c in common_columns]))
            )
            
            expected_with_hash = expected_subset.withColumn(
                "row_hash",
                md5(concat_ws("|", *[col(c).cast("string") for c in common_columns]))
            )
            
            # æ³¨å†Œä¸´æ—¶è¡¨è¿›è¡Œæ¯”è¾ƒ
            actual_with_hash.createOrReplaceTempView("actual_hashed")
            expected_with_hash.createOrReplaceTempView("expected_hashed")
            
            # æ‰¾å‡ºå·®å¼‚
            only_in_actual = self.spark.sql("""
                SELECT * FROM actual_hashed 
                WHERE row_hash NOT IN (SELECT row_hash FROM expected_hashed)
            """)
            
            only_in_expected = self.spark.sql("""
                SELECT * FROM expected_hashed 
                WHERE row_hash NOT IN (SELECT row_hash FROM actual_hashed)
            """)
            
            # ç»Ÿè®¡åŒ¹é…çš„è¡Œ
            matching_rows = self.spark.sql("""
                SELECT COUNT(*) as count FROM actual_hashed a
                INNER JOIN expected_hashed e ON a.row_hash = e.row_hash
            """).collect()[0]['count']
            
            content_comparison = {
                "actual_row_count": actual_count,
                "expected_row_count": expected_count,
                "matching_rows": matching_rows,
                "rows_only_in_actual": only_in_actual.count(),
                "rows_only_in_expected": only_in_expected.count(),
                "data_match_percentage": (matching_rows / max(actual_count, expected_count) * 100) if max(actual_count, expected_count) > 0 else 0,
                "sample_differences": {
                    "only_in_actual": only_in_actual.limit(10).toPandas().to_dict('records') if only_in_actual.count() > 0 else [],
                    "only_in_expected": only_in_expected.limit(10).toPandas().to_dict('records') if only_in_expected.count() > 0 else []
                }
            }
            
            return content_comparison
            
        except Exception as e:
            self.log_message(f"æ•°æ®å†…å®¹æ¯”è¾ƒå¤±è´¥: {str(e)}", "ERROR")
            return {"error": str(e)}
    
    def generate_column_statistics(self, df, table_name: str) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {}
            df.createOrReplaceTempView(f"stats_{table_name}")
            
            for column in df.columns:
                col_type = str(df.schema[column].dataType)
                
                if "string" in col_type.lower():
                    # å­—ç¬¦ä¸²åˆ—ç»Ÿè®¡
                    col_stats = self.spark.sql(f"""
                        SELECT 
                            COUNT(*) as total_count,
                            COUNT(`{column}`) as non_null_count,
                            COUNT(DISTINCT `{column}`) as distinct_count,
                            MIN(LENGTH(`{column}`)) as min_length,
                            MAX(LENGTH(`{column}`)) as max_length
                        FROM stats_{table_name}
                    """).collect()[0].asDict()
                
                elif any(x in col_type.lower() for x in ['int', 'long', 'float', 'double', 'decimal']):
                    # æ•°å€¼åˆ—ç»Ÿè®¡
                    col_stats = self.spark.sql(f"""
                        SELECT 
                            COUNT(*) as total_count,
                            COUNT(`{column}`) as non_null_count,
                            COUNT(DISTINCT `{column}`) as distinct_count,
                            MIN(`{column}`) as min_value,
                            MAX(`{column}`) as max_value,
                            AVG(`{column}`) as avg_value
                        FROM stats_{table_name}
                    """).collect()[0].asDict()
                
                else:
                    # å…¶ä»–ç±»å‹åŸºæœ¬ç»Ÿè®¡
                    col_stats = self.spark.sql(f"""
                        SELECT 
                            COUNT(*) as total_count,
                            COUNT(`{column}`) as non_null_count,
                            COUNT(DISTINCT `{column}`) as distinct_count
                        FROM stats_{table_name}
                    """).collect()[0].asDict()
                
                stats[column] = {
                    "data_type": col_type,
                    **col_stats
                }
            
            return stats
            
        except Exception as e:
            self.log_message(f"ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {str(e)}", "ERROR")
            return {}
    
    def validate_transformation(self, source_file: str, expected_file: str, spark_sql: str) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®è½¬æ¢"""
        self.log_message("å¼€å§‹æ•°æ®è½¬æ¢éªŒè¯æµç¨‹...")
        
        validation_results = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source_file": source_file,
            "expected_file": expected_file,
            "spark_sql": spark_sql,
            "success": False,
            "schema_comparison": {},
            "content_comparison": {},
            "statistics": {},
            "summary": {}
        }
        
        try:
            # 1. è¯»å–æºæ•°æ®æ–‡ä»¶
            if not self.read_csv_file(source_file, "source_table"):
                validation_results["error"] = "æ— æ³•è¯»å–æºæ•°æ®æ–‡ä»¶"
                return validation_results
            
            # 2. æ‰§è¡ŒSpark SQLè½¬æ¢
            actual_df = self.execute_spark_sql(spark_sql)
            if actual_df is None:
                validation_results["error"] = "Spark SQLæ‰§è¡Œå¤±è´¥"
                return validation_results
            
            # 3. åŠ è½½æœŸæœ›ç»“æœ
            expected_df = self.load_expected_results(expected_file)
            if expected_df is None:
                validation_results["error"] = "æ— æ³•åŠ è½½æœŸæœ›ç»“æœæ–‡ä»¶"
                return validation_results
            
            # 4. æ¯”è¾ƒæ•°æ®ç»“æ„
            schema_comparison = self.compare_schemas(actual_df, expected_df)
            validation_results["schema_comparison"] = schema_comparison
            
            # 5. æ¯”è¾ƒæ•°æ®å†…å®¹ï¼ˆä»…æ¯”è¾ƒå…±åŒåˆ—ï¼‰
            if schema_comparison["common_columns"]:
                content_comparison = self.compare_data_content(
                    actual_df, expected_df, schema_comparison["common_columns"]
                )
                validation_results["content_comparison"] = content_comparison
            
            # 6. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            validation_results["statistics"] = {
                "actual_data": self.generate_column_statistics(actual_df, "actual"),
                "expected_data": self.generate_column_statistics(expected_df, "expected")
            }
            
            # 7. ç”ŸæˆéªŒè¯æ‘˜è¦
            is_schema_match = (
                len(schema_comparison["missing_columns"]) == 0 and
                len(schema_comparison["extra_columns"]) == 0 and
                len(schema_comparison["data_type_mismatches"]) == 0
            )
            
            is_content_match = False
            if "content_comparison" in validation_results and "error" not in validation_results["content_comparison"]:
                content_comp = validation_results["content_comparison"]
                is_content_match = (
                    content_comp.get("data_match_percentage", 0) == 100.0 and
                    content_comp.get("rows_only_in_actual", 0) == 0 and
                    content_comp.get("rows_only_in_expected", 0) == 0
                )
            
            validation_results["summary"] = {
                "overall_success": is_schema_match and is_content_match,
                "schema_match": is_schema_match,
                "content_match": is_content_match,
                "match_percentage": validation_results.get("content_comparison", {}).get("data_match_percentage", 0)
            }
            
            validation_results["success"] = True
            self.log_message("éªŒè¯æµç¨‹å®Œæˆ")
            
        except Exception as e:
            self.log_message(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}", "ERROR")
            validation_results["error"] = str(e)
        
        return validation_results
    
    def generate_html_report(self, validation_results: Dict[str, Any], output_path: str = "validation_report.html"):
        """ç”ŸæˆHTMLéªŒè¯æŠ¥å‘Š"""
        try:
            html_template = """
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Spark SQL éªŒè¯æŠ¥å‘Š</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                    .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                    .header {{ text-align: center; margin-bottom: 30px; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 8px; }}
                    .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 30px; }}
                    .summary-card {{ padding: 20px; border-radius: 8px; text-align: center; color: white; }}
                    .success {{ background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%); }}
                    .warning {{ background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); }}
                    .info {{ background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); }}
                    .section {{ margin-bottom: 30px; }}
                    .section-title {{ font-size: 1.5em; font-weight: bold; margin-bottom: 15px; padding: 10px; background: #f8f9fa; border-left: 4px solid #007bff; }}
                    table {{ width: 100%; border-collapse: collapse; margin-bottom: 20px; }}
                    th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                    th {{ background-color: #f8f9fa; font-weight: bold; }}
                    .highlight {{ background-color: #fff3cd; }}
                    .error {{ background-color: #f8d7da; }}
                    .success-row {{ background-color: #d4edda; }}
                    .code-block {{ background: #f8f9fa; padding: 15px; border-radius: 4px; font-family: monospace; white-space: pre-wrap; overflow-x: auto; border-left: 4px solid #007bff; }}
                    .log-entry {{ padding: 5px; border-bottom: 1px solid #eee; font-family: monospace; font-size: 0.9em; }}
                    .log-error {{ color: #dc3545; }}
                    .log-info {{ color: #17a2b8; }}
                </style>
            </head>
            <body>
                <div class="container">
                    <div class="header">
                        <h1>ğŸ” Spark SQL éªŒè¯æŠ¥å‘Š</h1>
                        <p>ç”Ÿæˆæ—¶é—´: {timestamp}</p>
                    </div>
                    
                    <div class="summary">
                        <div class="summary-card {overall_status}">
                            <h3>æ€»ä½“çŠ¶æ€</h3>
                            <h2>{overall_result}</h2>
                        </div>
                        <div class="summary-card {schema_status}">
                            <h3>ç»“æ„åŒ¹é…</h3>
                            <h2>{schema_result}</h2>
                        </div>
                        <div class="summary-card {content_status}">
                            <h3>å†…å®¹åŒ¹é…åº¦</h3>
                            <h2>{content_percentage:.1f}%</h2>
                        </div>
                    </div>
                    
                    <div class="section">
                        <div class="section-title">ğŸ“‹ åŸºæœ¬ä¿¡æ¯</div>
                        <table>
                            <tr><th>æºæ•°æ®æ–‡ä»¶</th><td>{source_file}</td></tr>
                            <tr><th>æœŸæœ›ç»“æœæ–‡ä»¶</th><td>{expected_file}</td></tr>
                            <tr><th>æ‰§è¡Œæ—¶é—´</th><td>{timestamp}</td></tr>
                        </table>
                    </div>
                    
                    <div class="section">
                        <div class="section-title">ğŸ’¾ Spark SQL è¯­å¥</div>
                        <div class="code-block">{spark_sql}</div>
                    </div>
                    
                    {schema_section}
                    
                    {content_section}
                    
                    {statistics_section}
                    
                    {log_section}
                </div>
            </body>
            </html>
            """
            
            # å‡†å¤‡æ¨¡æ¿å˜é‡
            summary = validation_results.get("summary", {})
            
            overall_success = summary.get("overall_success", False)
            schema_match = summary.get("schema_match", False)
            content_percentage = summary.get("match_percentage", 0)
            
            template_vars = {
                "timestamp": validation_results.get("timestamp", ""),
                "source_file": validation_results.get("source_file", ""),
                "expected_file": validation_results.get("expected_file", ""),
                "spark_sql": validation_results.get("spark_sql", ""),
                "overall_status": "success" if overall_success else "warning",
                "overall_result": "âœ… é€šè¿‡" if overall_success else "âŒ å¤±è´¥",
                "schema_status": "success" if schema_match else "warning",
                "schema_result": "âœ… åŒ¹é…" if schema_match else "âŒ ä¸åŒ¹é…",
                "content_status": "success" if content_percentage == 100 else "warning" if content_percentage > 80 else "info",
                "content_percentage": content_percentage
            }
            
            # ç”Ÿæˆå„ä¸ªéƒ¨åˆ†çš„HTML
            template_vars["schema_section"] = self._generate_schema_section(validation_results.get("schema_comparison", {}))
            template_vars["content_section"] = self._generate_content_section(validation_results.get("content_comparison", {}))
            template_vars["statistics_section"] = self._generate_statistics_section(validation_results.get("statistics", {}))
            template_vars["log_section"] = self._generate_log_section()
            
            # ç”Ÿæˆæœ€ç»ˆHTML
            html_content = html_template.format(**template_vars)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            self.log_message(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
            
        except Exception as e:
            self.log_message(f"ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {str(e)}", "ERROR")
    
    def _generate_schema_section(self, schema_comparison: Dict) -> str:
        """ç”Ÿæˆç»“æ„æ¯”è¾ƒéƒ¨åˆ†çš„HTML"""
        if not schema_comparison:
            return ""
        
        html = '''
        <div class="section">
            <div class="section-title">ğŸ—ï¸ æ•°æ®ç»“æ„æ¯”è¾ƒ</div>
            <table>
                <tr><th>é¡¹ç›®</th><th>å®é™…ç»“æœ</th><th>æœŸæœ›ç»“æœ</th></tr>
                <tr><td>æ€»åˆ—æ•°</td><td>{}</td><td>{}</td></tr>
        '''.format(
            len(schema_comparison.get("actual_columns", [])),
            len(schema_comparison.get("expected_columns", []))
        )
        
        if schema_comparison.get("missing_columns"):
            html += f'<tr class="error"><td>ç¼ºå¤±åˆ—</td><td colspan="2">{", ".join(schema_comparison["missing_columns"])}</td></tr>'
        
        if schema_comparison.get("extra_columns"):
            html += f'<tr class="warning"><td>é¢å¤–åˆ—</td><td colspan="2">{", ".join(schema_comparison["extra_columns"])}</td></tr>'
        
        html += '</table>'
        
        if schema_comparison.get("data_type_mismatches"):
            html += '<h4>æ•°æ®ç±»å‹ä¸åŒ¹é…</h4><table><tr><th>åˆ—å</th><th>å®é™…ç±»å‹</th><th>æœŸæœ›ç±»å‹</th></tr>'
            for col, types in schema_comparison["data_type_mismatches"].items():
                html += f'<tr class="error"><td>{col}</td><td>{types["actual"]}</td><td>{types["expected"]}</td></tr>'
            html += '</table>'
        
        html += '</div>'
        return html
    
    def _generate_content_section(self, content_comparison: Dict) -> str:
        """ç”Ÿæˆå†…å®¹æ¯”è¾ƒéƒ¨åˆ†çš„HTML"""
        if not content_comparison:
            return ""
        
        html = f'''
        <div class="section">
            <div class="section-title">ğŸ“Š æ•°æ®å†…å®¹æ¯”è¾ƒ</div>
            <table>
                <tr><th>æŒ‡æ ‡</th><th>æ•°å€¼</th></tr>
                <tr><td>å®é™…æ•°æ®è¡Œæ•°</td><td>{content_comparison.get("actual_row_count", 0)}</td></tr>
                <tr><td>æœŸæœ›æ•°æ®è¡Œæ•°</td><td>{content_comparison.get("expected_row_count", 0)}</td></tr>
                <tr><td>åŒ¹é…è¡Œæ•°</td><td>{content_comparison.get("matching_rows", 0)}</td></tr>
                <tr><td>åŒ¹é…ç™¾åˆ†æ¯”</td><td>{content_comparison.get("data_match_percentage", 0):.2f}%</td></tr>
                <tr><td>ä»…å­˜åœ¨äºå®é™…ç»“æœ</td><td>{content_comparison.get("rows_only_in_actual", 0)}</td></tr>
                <tr><td>ä»…å­˜åœ¨äºæœŸæœ›ç»“æœ</td><td>{content_comparison.get("rows_only_in_expected", 0)}</td></tr>
            </table>
        </div>
        '''
        return html
    
    def _generate_statistics_section(self, statistics: Dict) -> str:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†çš„HTML"""
        if not statistics:
            return ""
        
        html = '<div class="section"><div class="section-title">ğŸ“ˆ æ•°æ®ç»Ÿè®¡</div>'
        
        for table_name, stats in statistics.items():
            html += f'<h4>{table_name}</h4><table><tr><th>åˆ—å</th><th>æ•°æ®ç±»å‹</th><th>æ€»æ•°</th><th>éç©ºæ•°</th><th>å”¯ä¸€å€¼æ•°</th></tr>'
            for col, col_stats in stats.items():
                html += f'''<tr>
                    <td>{col}</td>
                    <td>{col_stats.get("data_type", "")}</td>
                    <td>{col_stats.get("total_count", "")}</td>
                    <td>{col_stats.get("non_null_count", "")}</td>
                    <td>{col_stats.get("distinct_count", "")}</td>
                </tr>'''
            html += '</table>'
        
        html += '</div>'
        return html
    
    def _generate_log_section(self) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ—¥å¿—éƒ¨åˆ†çš„HTML"""
        html = '<div class="section"><div class="section-title">ğŸ“ æ‰§è¡Œæ—¥å¿—</div>'
        for log_entry in self.execution_log:
            css_class = "log-error" if "[ERROR]" in log_entry else "log-info" if "[INFO]" in log_entry else ""
            html += f'<div class="log-entry {css_class}">{log_entry}</div>'
        html += '</div>'
        return html
    
    def close(self):
        """å…³é—­Sparkä¼šè¯"""
        if hasattr(self, 'spark'):
            self.spark.stop()

def main():
    parser = argparse.ArgumentParser(description="Spark SQLéªŒè¯å™¨ - æ¯”å¯¹æ•°æ®è½¬æ¢ç»“æœ")
    parser.add_argument("--source", "-s", required=True, help="æºæ•°æ®CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--expected", "-e", required=True, help="æœŸæœ›ç»“æœCSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sql", "-q", required=True, help="Spark SQLè¯­å¥æˆ–SQLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", default="validation_report.html", help="HTMLæŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å‚æ•°
    print(f"æºæ•°æ®æ–‡ä»¶: {args.source}")
    print(f"æœŸæœ›ç»“æœæ–‡ä»¶: {args.expected}")
    print(f"SQLå‚æ•°: {args.sql}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.source):
        print(f"é”™è¯¯: æºæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.source}")
        sys.exit(1)
    
    if not os.path.exists(args.expected):
        print(f"é”™è¯¯: æœŸæœ›ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {args.expected}")
        sys.exit(1)
    
    # åˆ›å»ºéªŒè¯å™¨å®ä¾‹
    validator = None
    
    validator = SparkSQLValidator()
    
    # åˆ¤æ–­SQLå‚æ•°æ˜¯æ–‡ä»¶è¿˜æ˜¯SQLè¯­å¥
    if os.path.exists(args.sql):
        # ä»æ–‡ä»¶è¯»å–SQL
        sql_query = validator.read_sql_from_file(args.sql)
    else:
        # ç›´æ¥ä½¿ç”¨SQLè¯­å¥
        sql_query = args.sql
    
    # æ‰§è¡ŒéªŒè¯
    results = validator.validate_transformation(
        source_file=args.source,
        expected_file=args.expected,
        spark_sql=sql_query
    )
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    validator.generate_html_report(results, args.output)
        
